#!/usr/bin/env bash
# mindbender-search -- Creates and maintains search index
# 
# To launch the search GUI:
# $ mindbender search gui
# 
# To update the search index with new searchable data produced by DeepDive, run:
# $ mindbender search update
# You can specify which searchable entities to update:
# $ mindbender search update [ENTITY]...
# 
# To check the search index status:
# $ mindbender search status
# 
# For any reason, to destroy the search index, run:
# $ mindbender search destroy
#
##
# Author: Jaeho Shin <netj@cs.stanford.edu>
# Created: 2015-07-31
set -eu

# needs to work on a DeepDive app
DEEPDIVE_APP=$(find-deepdive-app)
export DEEPDIVE_APP
cd "$DEEPDIVE_APP"

# use the DeepDive app's folder name for the ES index name
: ${ELASTICSEARCH_INDEX_NAME:=$(basename "$DEEPDIVE_APP")}
: ${ELASTICSEARCH_BULK_BATCHSIZE:=1000}

# parse command-line args
case ${1:-} in
    gui|update|status|destroy)
        # make sure elasticsearch is running
        ${ELASTICSEARCH_RUNNING:-false} ||
            exec keep-elasticsearch-during -- "$0" "$@"
        Action=$1; shift
        ;;
    *)
        usage "$0" "No action given: gui, update, status, or destroy"
esac

# simple wrapper for elasticsearch HTTP API
esAPI() {
    local verb=$1 path=$2; shift 2
    curl -fs -X$verb "$ELASTICSEARCH_BASEURL$path" "$@"
    echo  # because ES does not put an EOL
}
# shorthand for APIs on the index
esAPIx() {
    local verb=$1 path=$2; shift 2
    esAPI "$verb" "/$ELASTICSEARCH_INDEX_NAME$path" "$@"
}
need_tmpdir() {
    if ! [[ -d "${tmpdir:-}" ]]; then
        tmpdir=$(mktemp -d ${TMPDIR:-/tmp}/mindbender-search.XXXXXXX)
        trap "rm -rf $tmpdir" EXIT
    fi
}
jqSchema() {
    if ! [[ -f "${DDLOG_SCHEMA_JSON:-}" ]]; then
        need_tmpdir
        DDLOG_SCHEMA_JSON="$tmpdir"/app.ddlog.json
        # TODO keep this schema exporting under deepdive command?
        ddlog export-schema app.ddlog >"$DDLOG_SCHEMA_JSON"
    fi
    export DDLOG_SCHEMA_JSON
    jqDDlog <"$DDLOG_SCHEMA_JSON" "$@"
}


# perform action on search index
case $Action in
    gui)
        # derive a schema for search frontend from DDlog
        need_tmpdir
        DDLOG_SEARCH_SCHEMA="$tmpdir"/search-frontend-schema.json
        jqSchema '[relations] | mindbenderSearchFrontendSchemaForRelations' >"$DDLOG_SEARCH_SCHEMA"
        export DDLOG_SEARCH_SCHEMA

        # launch the GUI
        exec mindbender-gui "$@"
        ;;

    update)
        need_tmpdir
        # use relations specified over command-line to filter target relations
        export DDLOG_RELATIONS_SELECTED
        if [[ $# -eq 0 ]]; then
            # default to all searchable entities discovered from DDlog
            eval set -- $(jqSchema 'relations | .name | @sh')
        else
            # check errors in given names of relations
            DDLOG_RELATIONS_SELECTED=$(
                printf '%s\n' "$@" | jq -R -s -c 'rtrimstr("\n") | split("\n")')
            badNames=$(jqSchema '
                env.DDLOG_RELATIONS_SELECTED | fromjson[] |
                select(
                    ([relationByName] | length == 0) or
                    (relationByName | isAnnotated([.name] | inside(["source", "extraction"])) | not)
                )
            ' -r)
            [[ -z "$badNames" ]] ||
                error "$badNames: must be a @source or @extraction relation"
        fi
        # filter down to only @source and @extraction relations
        DDLOG_RELATIONS_SELECTED=$(jqSchema '[
            relationsSelected |
            annotated([.name] | inside(["source", "extraction"])) |
            .name
        ]' -c)
        [[ "$DDLOG_RELATIONS_SELECTED" != "[]" ]] ||
            error "No @source or @extraction relations found"
        # show which ones will be indexed first
        jqSchema 'relationsSelected | "Will index relation \(.name)"' >&2

        # set up parent-child mapping based on @references annotations
        # (See: https://www.elastic.co/guide/en/elasticsearch/guide/current/parent-child-mapping.html)
        jqSchema '([ relationsSelected ] | elasticsearchMappingsForRelations)' |
        esAPIx PUT "/" --data-binary @- >/dev/null || true

        # bulk load to ES from DeepDive database
        update-relation() {
            local relation=$1; shift
            local sqlToUnload=$1; shift
            local jqToFormat=$1; shift
            echo >&2 "Indexing relation $relation"
            # unload data from database in json lines
            deepdive sql eval "$sqlToUnload" format=json |
            # show progress
            pv --line-mode --size="$(deepdive sql eval "SELECT COUNT(*) FROM ($sqlToUnload) r")" |
            # split records into multiple batches
            # TODO parallelize
            split --lines=$ELASTICSEARCH_BULK_BATCHSIZE --filter="$(escape-args-for-shell sh -euc '
                jqToFormat=$1 indexURL=$2
                # produce elasticsearch bulk load action for each JSON record
                jq -c "$jqToFormat" |
                # send them to elasticsearch
                curl -fs -XPUT "$indexURL/_bulk" --data-binary @- |
                # discard all result but the error field
                jq ".error"
                ' -- "$jqToFormat" "$ELASTICSEARCH_BASEURL/$ELASTICSEARCH_INDEX_NAME/$relation")" |
            # make sure there wasn't any error
            jq -e -s "any | not" >/dev/null
        }
        eval "$(jqSchema '
            # generate shell commands that index every selected relations
            relationsSelected |
            "update-relation \(.name | @sh) \(
                sqlForRelationNestingAssociated | @sh) \(
                jqForBulkLoadingRelationIntoElasticsearch | @sh
            )"
        ')"
        ;;

    status)
        esAPI GET '/_stats'
        ;;

    destroy)
        # delete the ES index
        esAPIx DELETE "/"
        ;;

    *)
        error "$Action: unknown action"
esac
